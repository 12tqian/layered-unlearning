{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments 1\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    GaussianMixture,\n",
    "    Gaussian,\n",
    "    LogisticModel,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts\n",
    "Below are scripts for training and evaluating our models. Relearning is included in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    device: str = \"cuda\",\n",
    "    n_classes: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for each class in the dataset.\n",
    "    \"\"\"\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        outputs = (outputs > 0.5).float()\n",
    "        # Calculate accuracy for each class\n",
    "        for i in range(n_classes):\n",
    "            class_mask = y == i\n",
    "            class_accuracy = (\n",
    "                (outputs[class_mask] == y[class_mask]).float().mean().item()\n",
    "            )\n",
    "            accuracies.append(class_accuracy)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    flip_mask: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    "    n_epochs: int = 1,\n",
    "    lr: float = 0.01,\n",
    "    batch_size: int = 32,\n",
    "    weight_decay: float = 0.01,\n",
    "    device: str = \"cuda\",\n",
    "    eps: float = 1e-8,\n",
    "    log_1_minus_p: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model using the given data and parameters.\n",
    "    log_1_minus_p: if True, we optimize log(1 - p), otherwise we do gradient ascent.\n",
    "    flip_mask: mask for the data points we want to flip in terms of leanr/unlearn.\n",
    "    mask: mask for the data points we want to use for training, used for relearning.\n",
    "    \"\"\"\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    flip_mask = flip_mask.to(device)\n",
    "\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "\n",
    "    if mask is not None:\n",
    "        X_train = X_train[mask]\n",
    "        y_train = y_train[mask]\n",
    "\n",
    "        flip_mask = flip_mask[mask]\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    dataloader = DataLoader(\n",
    "        list(zip(X_train, y_train, flip_mask)),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y, batch_flip_mask in (\n",
    "            pbar := tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            if log_1_minus_p:\n",
    "                # do 1 - p\n",
    "                outputs = torch.where(\n",
    "                    batch_flip_mask,\n",
    "                    1 - outputs,\n",
    "                    outputs,\n",
    "                )\n",
    "\n",
    "            loss = -(\n",
    "                batch_y * torch.log(outputs + eps)\n",
    "                + (1 - batch_y) * torch.log(1 - outputs + eps)\n",
    "            )\n",
    "\n",
    "            if not log_1_minus_p:\n",
    "                # gradient ascent\n",
    "                loss = torch.where(\n",
    "                    batch_flip_mask,\n",
    "                    -loss,\n",
    "                    loss,\n",
    "                )\n",
    "\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 2\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 5000\n",
    "dim = 2\n",
    "weight_decay = 1e-3\n",
    "quadratic_features = False\n",
    "eps = 1e-8\n",
    "log_1_minus_p = True\n",
    "\n",
    "classes = [\n",
    "    GaussianMixture(\n",
    "        classes=[\n",
    "            Gaussian(mu=torch.tensor([-4, 0]), cov=torch.eye(dim)),\n",
    "            Gaussian(mu=torch.tensor([-2, 0]), cov=torch.eye(dim)),\n",
    "        ],\n",
    "        weights=[0.5, 0.5],\n",
    "    ),\n",
    "    GaussianMixture(\n",
    "        classes=[\n",
    "            Gaussian(mu=torch.tensor([2, 0]), cov=torch.eye(dim)),\n",
    "            Gaussian(mu=torch.tensor([4, 0]), cov=torch.eye(dim)),\n",
    "        ],\n",
    "        weights=[0.5, 0.5],\n",
    "    ),\n",
    "]\n",
    "\n",
    "assert (\n",
    "    len(classes) == n_classes\n",
    "), \"Number of classes must match the number of classes in the model\"\n",
    "\n",
    "X = [classes[i].sample(n_samples) for i in range(n_classes)]\n",
    "y = [torch.tensor([i] * n_samples) for i in range(n_classes)]\n",
    "\n",
    "X = torch.cat(X, dim=0).float()\n",
    "y = torch.cat(y, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:01<00:00, 252.07it/s, loss=0.00359]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 373.38it/s, loss=0.0152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init [0.9850000143051147, 0.9905999898910522]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 379.69it/s, loss=0.378]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 374.13it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base [0.015599999576807022, 0.009200000204145908]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 370.23it/s, loss=0.123] \n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 378.99it/s, loss=0.0487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial [0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 367.65it/s, loss=0.0766]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 371.85it/s, loss=0.0531] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu [0.004799999762326479, 0.02579999901354313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [00:00<00:00, 365.94it/s, loss=0.171]\n",
      "Epoch 2/2: 100%|██████████| 157/157 [00:00<00:00, 370.37it/s, loss=0.0832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A [1.0, 0.47360000014305115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [00:00<00:00, 371.25it/s, loss=0.171]\n",
      "Epoch 2/2: 100%|██████████| 157/157 [00:00<00:00, 369.70it/s, loss=0.0734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B [0.5105999708175659, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [00:00<00:00, 374.74it/s, loss=0.836]\n",
      "Epoch 2/2: 100%|██████████| 157/157 [00:00<00:00, 384.32it/s, loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A [1.0, 0.025599999353289604]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [00:00<00:00, 374.06it/s, loss=0.321]\n",
      "Epoch 2/2: 100%|██████████| 157/157 [00:00<00:00, 373.83it/s, loss=0.0811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        quadratic_features=quadratic_features,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(model: nn.Module, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    y_train = y.clone()\n",
    "\n",
    "    mask_A = y_train == 0\n",
    "    mask_B = y_train == 1\n",
    "\n",
    "    flip_mask = torch.zeros_like(y_train, dtype=torch.bool)\n",
    "    # if not learning, then flip labels\n",
    "    if not learn_A:\n",
    "        flip_mask = flip_mask | mask_A\n",
    "    if not learn_B:\n",
    "        flip_mask = flip_mask | mask_B\n",
    "\n",
    "    # if relearning, then only do labels where things True\n",
    "    if relearn:\n",
    "        mask = torch.zeros_like(y_train, dtype=torch.bool)\n",
    "\n",
    "        if learn_A:\n",
    "            mask = mask | mask_A\n",
    "        if learn_B:\n",
    "            mask = mask | mask_B\n",
    "    else:\n",
    "        mask = torch.ones_like(y_train, dtype=torch.bool)\n",
    "\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y_train,\n",
    "        mask=mask,\n",
    "        flip_mask=flip_mask,\n",
    "        log_1_minus_p=log_1_minus_p,\n",
    "        eps=eps,\n",
    "        n_epochs=n_epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module):\n",
    "    return evaluate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        device=device,\n",
    "        n_classes=n_classes,\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    # scatter each class\n",
    "    for i in range(n_classes):\n",
    "        plt.scatter(\n",
    "            X[y == i, 0].cpu(),\n",
    "            X[y == i, 1].cpu(),\n",
    "            label=f\"Class {i}\",\n",
    "            alpha=0.6,\n",
    "            edgecolors=\"k\",\n",
    "            color=\"red\" if i == 1 else \"blue\",\n",
    "        )\n",
    "    bias = model.linear.bias.detach()\n",
    "    weights = model.linear.weight.detach()\n",
    "    # plot decision boundary\n",
    "\n",
    "    x1 = torch.linspace(x_min, x_max, n_grid).to(device).detach()\n",
    "    x2 = (-(weights[0, 0] * x1 + bias) / weights[0, 1]).to(device).detach()\n",
    "    print(f\"{name}, Weights: {weights.cpu().numpy()}, Bias: {bias.cpu().numpy()}\")\n",
    "\n",
    "    plt.plot(\n",
    "        x1.cpu(), x2.cpu(), color=\"black\", linestyle=\"--\", label=\"Decision Boundary\"\n",
    "    )\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    evals[end] = global_eval(model)\n",
    "    print(end, evals[end])\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/tcqian/layered-unlearning/venv/lib/python3.12/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init, Weights: [[2.3036799e+00 4.4678618e-05]], Bias: [-0.00576402]\n",
      "base, Weights: [[-8.7733680e-01 -1.7904336e-04]], Bias: [-0.01507458]\n",
      "base-lu-partial, Weights: [[ 0.00901618 -0.01423081]], Bias: [2.9971185]\n",
      "base-lu, Weights: [[-1.7785853e+00  1.2855111e-03]], Bias: [0.65959495]\n",
      "base-A, Weights: [[0.472338   0.00715847]], Bias: [-1.4685963]\n",
      "base-B, Weights: [[ 0.48212668 -0.00402862]], Bias: [1.4425591]\n",
      "base-lu-A, Weights: [[0.23851372 0.00209444]], Bias: [-1.3599821]\n",
      "base-lu-B, Weights: [[ 0.02227729 -0.01541599]], Bias: [2.4209669]\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"./gmm1_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "    )\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
