{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments \n",
    "\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    GaussianMixture,\n",
    "    LogisticModel,\n",
    "    Uniform,\n",
    "    train,\n",
    "    evaluate,\n",
    "    construct_dataset,\n",
    ")\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4823/3560430921.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  filtered = torch.tensor(filtered)\n",
      "/mnt/align4_drive/tcqian/layered-unlearning/src/layered_unlearning/gmm_classification/dataset.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.weights), n_samples, replacement=True\n",
      "/mnt/align4_drive/tcqian/layered-unlearning/src/layered_unlearning/gmm_classification/dataset.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.weights), n_samples, replacement=True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 3\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 5000\n",
    "dim = 2\n",
    "weight_decay = 0.0\n",
    "weight_delta_penalty = 0.0\n",
    "\n",
    "rbf = True\n",
    "rbf_sigma = 8\n",
    "rbf_width = 60\n",
    "rbf_num = 12\n",
    "degree = 0\n",
    "\n",
    "eps = 1e-8\n",
    "n_layers = 0\n",
    "batch_norm = True\n",
    "hidden_dim = 128\n",
    "\n",
    "loss_type = \"cross_entropy\"\n",
    "\n",
    "cov_scale = 4\n",
    "cov_perturb = 0.1\n",
    "mu_width = 50\n",
    "uniform_half_width = 60\n",
    "n_classes = 5\n",
    "clustering = \"k-means\"  # random, k-means, adversarial\n",
    "\n",
    "\n",
    "def mu_gen():\n",
    "    return torch.rand((dim,)) * 2 * mu_width - mu_width\n",
    "\n",
    "\n",
    "def cov_gen():\n",
    "    base = torch.eye(dim) * cov_scale\n",
    "    U = torch.randn((dim, dim))\n",
    "    perturb = U.T @ U * cov_perturb\n",
    "    return base + perturb\n",
    "\n",
    "\n",
    "def get_gaussian_mixture(\n",
    "    n_classes: int,\n",
    "    mu_list: List[torch.Tensor] = None,\n",
    "    cov_list: List[torch.Tensor] = None,\n",
    ") -> GaussianMixture:\n",
    "    classes = []\n",
    "    for i in range(n_classes):\n",
    "        classes.append(\n",
    "            Gaussian(\n",
    "                mu=mu_gen() if mu_list is None else mu_list[i],\n",
    "                cov=cov_gen() if cov_list is None else cov_list[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    mixture = GaussianMixture(\n",
    "        classes=classes,\n",
    "        weights=torch.ones(n_classes) / n_classes,\n",
    "    )\n",
    "    return mixture\n",
    "\n",
    "\n",
    "def get_even_clusters(X: np.ndarray, cluster_size: int):\n",
    "    n_clusters = int(np.ceil(len(X) / cluster_size))\n",
    "    kmeans = KMeans(n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    centers = (\n",
    "        centers.reshape(-1, 1, X.shape[-1])\n",
    "        .repeat(cluster_size, 1)\n",
    "        .reshape(-1, X.shape[-1])\n",
    "    )\n",
    "    distance_matrix = cdist(X, centers)\n",
    "    clusters = linear_sum_assignment(distance_matrix)[1] // cluster_size\n",
    "    return clusters\n",
    "\n",
    "\n",
    "mean_lists = []\n",
    "for i in range(3):\n",
    "    for j in range(n_classes):\n",
    "        mean_lists.append(mu_gen())\n",
    "if clustering == \"random\":\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3, n_classes, dim)\n",
    "elif clustering == \"k-means\":\n",
    "    all_means = torch.cat(mean_lists, dim=0)\n",
    "    all_means = all_means.reshape(-1, dim)\n",
    "    labels = get_even_clusters(all_means.numpy(), n_classes)\n",
    "    mean_lists = []\n",
    "    for i in range(3):\n",
    "        filtered = all_means[labels == i]\n",
    "        filtered = torch.tensor(filtered)\n",
    "        mean_lists.append(filtered)\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3, n_classes, dim)\n",
    "elif clustering == \"adversarial\":\n",
    "    all_means = torch.cat(mean_lists, dim=0)\n",
    "    all_means = all_means.reshape(-1, dim)\n",
    "    labels = get_even_clusters(all_means.numpy(), n_classes)\n",
    "    better_means = [[] for _ in range(3)]\n",
    "    mean_lists = []\n",
    "    for i in range(3):\n",
    "        filtered = all_means[labels == i]\n",
    "        filtered = torch.tensor(filtered)\n",
    "        mean_lists.append(filtered)\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3 * n_classes, dim)\n",
    "    for i in range(3 * n_classes):\n",
    "        better_means[i % 3].append(mean_lists[i])\n",
    "    for i in range(3):\n",
    "        better_means[i] = torch.stack(better_means[i])\n",
    "    mean_lists = better_means\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3, n_classes, dim)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown clustering method: {clustering}\")\n",
    "\n",
    "gaussians = [\n",
    "    Uniform(\n",
    "        low=torch.tensor([-1.0, -1.0]) * uniform_half_width,\n",
    "        high=torch.tensor([1.0, 1.0]) * uniform_half_width,\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "    gaussians.append(\n",
    "        get_gaussian_mixture(\n",
    "            n_classes=n_classes,\n",
    "            mu_list=mean_lists[i],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 440.20it/s, loss=0.229] \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 563.46it/s, loss=0.322] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 533.61it/s, loss=0.34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init, A: 1.00, B: 1.00, Retain: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 442.65it/s, loss=0.13]  \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 476.00it/s, loss=0.12]  \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 487.03it/s, loss=0.0319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base, A: 0.00, B: 0.00, Retain: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 490.52it/s, loss=0.172] \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 492.19it/s, loss=0.0281]\n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 475.73it/s, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial, A: 0.00, B: 1.00, Retain: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 481.75it/s, loss=0.0459]\n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 487.76it/s, loss=0.115] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 486.90it/s, loss=0.0331] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu, A: 0.00, B: 0.00, Retain: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 478.71it/s, loss=0.676]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 496.66it/s, loss=0.14]  \n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 500.31it/s, loss=0.0315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A, A: 1.00, B: 0.60, Retain: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 495.93it/s, loss=0.536]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 501.71it/s, loss=0.0922]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 500.39it/s, loss=0.0697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B, A: 0.74, B: 1.00, Retain: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 496.55it/s, loss=6.18]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 506.37it/s, loss=1]    \n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 508.02it/s, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A, A: 0.92, B: 0.66, Retain: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 491.74it/s, loss=0.59] \n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 497.47it/s, loss=0.161] \n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 499.16it/s, loss=0.0574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B, A: 0.32, B: 1.00, Retain: 0.90\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        degree=degree,\n",
    "        rbf=rbf,\n",
    "        rbf_sigma=rbf_sigma,\n",
    "        rbf_width=rbf_width,\n",
    "        rbf_num=rbf_num,\n",
    "        n_layers=n_layers,\n",
    "        batch_norm=batch_norm,\n",
    "        hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(\n",
    "    model: nn.Module,\n",
    "    learn_A: bool,\n",
    "    learn_B: bool,\n",
    "    relearn: bool = False,\n",
    "    kwargs: Dict = {},\n",
    "):\n",
    "    X, y = construct_dataset(\n",
    "        X_full, learn_A=learn_A, learn_B=learn_B, relearn=relearn, n_samples=n_samples\n",
    "    )\n",
    "    init_kwargs = {\n",
    "        \"eps\": eps,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"device\": device,\n",
    "        \"loss_type\": loss_type,\n",
    "    }\n",
    "    init_kwargs.update(kwargs)\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        **init_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module, kwargs: Dict = {}):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        if i == 3:\n",
    "            X = torch.cat([X_full[0], X])\n",
    "            y = torch.cat([torch.zeros(n_samples), y])\n",
    "        acc = evaluate(model, X, y, device=device, batch_size=batch_size, **kwargs)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def run(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    learn_A: bool,\n",
    "    learn_B: bool,\n",
    "    relearn: bool = False,\n",
    "    train_kwargs: Dict = {},\n",
    "    eval_kwargs: Dict = {},\n",
    "):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(\n",
    "        model, learn_A=learn_A, learn_B=learn_B, relearn=relearn, kwargs=train_kwargs\n",
    "    )\n",
    "    evals[end] = global_eval(model, kwargs=eval_kwargs)\n",
    "    print(\n",
    "        f\"{end}, A: {evals[end][0]:.2f}, B: {evals[end][1]:.2f}, Retain: {evals[end][2]:.2f}\"\n",
    "    )\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str, train_kwargs: Dict = {}, eval_kwargs: Dict = {}):\n",
    "    run(\n",
    "        name,\n",
    "        f\"{name}-A\",\n",
    "        learn_A=True,\n",
    "        learn_B=False,\n",
    "        relearn=True,\n",
    "        train_kwargs=train_kwargs,\n",
    "        eval_kwargs=eval_kwargs,\n",
    "    )\n",
    "    run(\n",
    "        name,\n",
    "        f\"{name}-B\",\n",
    "        learn_A=False,\n",
    "        learn_B=True,\n",
    "        relearn=True,\n",
    "        train_kwargs=train_kwargs,\n",
    "        eval_kwargs=eval_kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = [\n",
    "    {\n",
    "        \"name\": name,\n",
    "        \"A\": result[0],\n",
    "        \"B\": result[1],\n",
    "        \"retain\": result[2],\n",
    "    }\n",
    "    for name, result in evals.items()\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set consistent fonts and style globally\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\", \"Times New Roman\", \"serif\"]\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"axes.titlesize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    name: str,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "    include_scatter: bool = True,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "\n",
    "    # Plot limits\n",
    "    width = 60\n",
    "    x_min, x_max = -width, width\n",
    "    y_min, y_max = -width, width\n",
    "\n",
    "    # Meshgrid\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "        indexing=\"ij\",  # Required for compatibility with torch >= 1.10\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    # Plot decision boundary\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    contour = ax.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    # Helper scatter function\n",
    "    def scatter(\n",
    "        x: torch.Tensor, y: torch.Tensor, color: str, label: str, n_samples: int = 1000\n",
    "    ):\n",
    "        if x.shape[0] > n_samples:\n",
    "            indices = torch.randperm(x.shape[0])[:n_samples]\n",
    "            x, y = x[indices], y[indices]\n",
    "        ax.scatter(x.cpu(), y.cpu(), s=4, color=color, label=label, alpha=0.5)\n",
    "\n",
    "    if include_scatter:\n",
    "        frac = 0.25\n",
    "        small_samples = int(n_samples * frac)\n",
    "        scatter(\n",
    "            X_full[0][:, 0],\n",
    "            X_full[0][:, 1],\n",
    "            color=\"blue\",\n",
    "            label=\"Null\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        scatter(\n",
    "            X_full[1][:, 0],\n",
    "            X_full[1][:, 1],\n",
    "            color=\"orange\",\n",
    "            label=\"A\",\n",
    "            n_samples=small_samples,\n",
    "        )\n",
    "        scatter(\n",
    "            X_full[2][:, 0],\n",
    "            X_full[2][:, 1],\n",
    "            color=\"gold\",\n",
    "            label=\"B\",\n",
    "            n_samples=small_samples,\n",
    "        )\n",
    "        scatter(\n",
    "            X_full[3][:, 0],\n",
    "            X_full[3][:, 1],\n",
    "            color=\"red\",\n",
    "            label=\"Retain\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "\n",
    "    # Format plot\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel(\"Feature 1\", fontsize=20, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Feature 2\", fontsize=20, fontweight=\"bold\")\n",
    "\n",
    "    if include_scatter:\n",
    "        # Create scatter legend\n",
    "        scatter_handles, scatter_labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Create contour legend entries manually\n",
    "        contour_handles = [\n",
    "            Patch(facecolor=\"blue\", alpha=0.2, label=\"Class 0\"),\n",
    "            Patch(facecolor=\"red\", alpha=0.2, label=\"Class 1\"),\n",
    "        ]\n",
    "\n",
    "        # Combine legends\n",
    "        ax.legend(\n",
    "            handles=contour_handles + scatter_handles,\n",
    "            title=\"Legend\",\n",
    "            fontsize=16,\n",
    "            title_fontsize=16,\n",
    "            frameon=True,\n",
    "            framealpha=0.9,\n",
    "            loc=\"upper right\",\n",
    "        )\n",
    "\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", width=1, length=5)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "base_dir = Path(\"./gmm_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(X_full, learn_A=True, learn_B=True, n_samples=n_samples)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        n_grid=100,\n",
    "        n_samples=1000,\n",
    "        output_path=base_dir / f\"{name}.svg\",\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
