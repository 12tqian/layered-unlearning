{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments \n",
    "\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    GaussianMixture,\n",
    "    LogisticModel,\n",
    "    Uniform,\n",
    "    train,\n",
    "    evaluate,\n",
    "    construct_dataset,\n",
    ")\n",
    "import math\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "seed = set_seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_631183/4239274528.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  filtered = torch.tensor(filtered)\n",
      "/mnt/align4_drive/tcqian/layered-unlearning/src/layered_unlearning/gmm_classification/dataset.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.weights), n_samples, replacement=True\n",
      "/mnt/align4_drive/tcqian/layered-unlearning/src/layered_unlearning/gmm_classification/dataset.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.weights), n_samples, replacement=True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 3\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 5000\n",
    "dim = 2\n",
    "weight_decay = 0.0\n",
    "weight_delta_penalty = 0.0\n",
    "\n",
    "rbf = True\n",
    "degree = 0\n",
    "\n",
    "eps = 1e-8\n",
    "n_layers = 0\n",
    "batch_norm = True\n",
    "hidden_dim = 128\n",
    "\n",
    "loss_type = \"cross_entropy\"\n",
    "\n",
    "\n",
    "def ellipse(rotate: float = 0.0, x_scale: float = 1.0, y_scale: float = 1.0):\n",
    "    rotate = rotate * (torch.pi / 180)\n",
    "    cov = torch.Tensor(\n",
    "        [\n",
    "            [x_scale, 0],\n",
    "            [0, y_scale],\n",
    "        ]\n",
    "    )\n",
    "    rotate = torch.Tensor(\n",
    "        [\n",
    "            [math.cos(rotate), -math.sin(rotate)],\n",
    "            [math.sin(rotate), math.cos(rotate)],\n",
    "        ]\n",
    "    )\n",
    "    return rotate @ cov @ rotate.T\n",
    "\n",
    "\n",
    "def mu_gen():\n",
    "    width = 50\n",
    "    return torch.rand((dim,)) * 2 * width - width\n",
    "\n",
    "\n",
    "def cov_gen():\n",
    "    base = torch.eye(dim) * 4\n",
    "    U = torch.randn((dim, dim))\n",
    "    perturb = U.T @ U * 0.1\n",
    "    return base + perturb\n",
    "\n",
    "\n",
    "def get_gaussian_mixture(\n",
    "    n_classes: int,\n",
    "    mu_list: List[torch.Tensor] = None,\n",
    "    cov_list: List[torch.Tensor] = None,\n",
    ") -> GaussianMixture:\n",
    "    classes = []\n",
    "    for i in range(n_classes):\n",
    "        classes.append(\n",
    "            Gaussian(\n",
    "                mu=mu_gen() if mu_list is None else mu_list[i],\n",
    "                cov=cov_gen() if cov_list is None else cov_list[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    mixture = GaussianMixture(\n",
    "        classes=classes,\n",
    "        weights=torch.ones(n_classes) / n_classes,\n",
    "    )\n",
    "    return mixture\n",
    "\n",
    "\n",
    "def get_even_clusters(X: np.ndarray, cluster_size: int):\n",
    "    n_clusters = int(np.ceil(len(X) / cluster_size))\n",
    "    kmeans = KMeans(n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    centers = (\n",
    "        centers.reshape(-1, 1, X.shape[-1])\n",
    "        .repeat(cluster_size, 1)\n",
    "        .reshape(-1, X.shape[-1])\n",
    "    )\n",
    "    distance_matrix = cdist(X, centers)\n",
    "    clusters = linear_sum_assignment(distance_matrix)[1] // cluster_size\n",
    "    return clusters\n",
    "\n",
    "\n",
    "uniform_half_width = 60\n",
    "n_classes = 3\n",
    "clustering = \"adversarial\"  # random, k-means, adversarial\n",
    "\n",
    "mean_lists = []\n",
    "for i in range(3):\n",
    "    for j in range(n_classes):\n",
    "        mean_lists.append(mu_gen())\n",
    "if clustering == \"random\":\n",
    "    # reshape to (3, n_classes, dim)\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3, n_classes, dim)\n",
    "elif clustering == \"k-means\":\n",
    "    all_means = torch.cat(mean_lists, dim=0)\n",
    "    all_means = all_means.reshape(-1, dim)\n",
    "    # get clusters\n",
    "    labels = get_even_clusters(all_means.numpy(), n_classes)\n",
    "    # reshape to (3, n_classes, dim)\n",
    "    # group by labels\n",
    "    mean_lists = []\n",
    "    for i in range(3):\n",
    "        filtered = all_means[labels == i]\n",
    "        # convert to tensor\n",
    "        filtered = torch.tensor(filtered)\n",
    "        mean_lists.append(filtered)\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3, n_classes, dim)\n",
    "elif clustering == \"adversarial\":\n",
    "    all_means = torch.cat(mean_lists, dim=0)\n",
    "    all_means = all_means.reshape(-1, dim)\n",
    "    # get clusters\n",
    "    labels = get_even_clusters(all_means.numpy(), n_classes)\n",
    "    # reshape to (3, n_classes, dim)\n",
    "    # group by labels\n",
    "    better_means = [[] for _ in range(3)]\n",
    "    mean_lists = []\n",
    "    for i in range(3):\n",
    "        filtered = all_means[labels == i]\n",
    "        # convert to tensor\n",
    "        filtered = torch.tensor(filtered)\n",
    "        mean_lists.append(filtered)\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3 * n_classes, dim)\n",
    "    for i in range(3 * n_classes):\n",
    "        better_means[i % 3].append(mean_lists[i])\n",
    "    for i in range(3):\n",
    "        better_means[i] = torch.stack(better_means[i])\n",
    "    mean_lists = better_means\n",
    "    mean_lists = torch.stack(mean_lists).reshape(3, n_classes, dim)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown clustering method: {clustering}\")\n",
    "\n",
    "gaussians = [\n",
    "    Uniform(\n",
    "        low=torch.tensor([-1.0, -1.0]) * uniform_half_width,\n",
    "        high=torch.tensor([1.0, 1.0]) * uniform_half_width,\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "    gaussians.append(\n",
    "        get_gaussian_mixture(\n",
    "            n_classes=n_classes,\n",
    "            mu_list=mean_lists[i],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/625 [00:00<?, ?it/s, loss=0.591]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 518.93it/s, loss=0.16]  \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 604.12it/s, loss=0.251] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 590.10it/s, loss=0.211] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init, A: 1.00, B: 1.00, Retain: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 598.93it/s, loss=0.0294]\n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 593.54it/s, loss=0.017]  \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 593.10it/s, loss=0.0763] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base, A: 0.00, B: 0.00, Retain: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 580.89it/s, loss=0.129] \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 574.39it/s, loss=0.0277]\n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 576.65it/s, loss=0.0275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial, A: 0.00, B: 1.00, Retain: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 579.89it/s, loss=0.0719] \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 578.06it/s, loss=0.0103] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 594.26it/s, loss=0.026]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu, A: 0.00, B: 0.00, Retain: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 587.17it/s, loss=0.576]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 590.84it/s, loss=0.0833]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 587.37it/s, loss=0.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A, A: 1.00, B: 0.68, Retain: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 589.55it/s, loss=0.518]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 587.38it/s, loss=0.0795]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 588.60it/s, loss=0.0544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B, A: 0.60, B: 1.00, Retain: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 582.98it/s, loss=4.98]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 586.87it/s, loss=0.352]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 581.71it/s, loss=0.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A, A: 1.00, B: 0.68, Retain: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 583.99it/s, loss=0.471]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 587.06it/s, loss=0.0996]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 588.27it/s, loss=0.058] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B, A: 0.00, B: 1.00, Retain: 0.94\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        degree=degree,\n",
    "        rbf=rbf,\n",
    "        n_layers=n_layers,\n",
    "        batch_norm=batch_norm,\n",
    "        hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(\n",
    "    model: nn.Module,\n",
    "    learn_A: bool,\n",
    "    learn_B: bool,\n",
    "    relearn: bool = False,\n",
    "    kwargs: Dict = {},\n",
    "):\n",
    "    X, y = construct_dataset(\n",
    "        X_full, learn_A=learn_A, learn_B=learn_B, relearn=relearn, n_samples=n_samples\n",
    "    )\n",
    "    init_kwargs = {\n",
    "        \"eps\": eps,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"device\": device,\n",
    "        \"loss_type\": loss_type,\n",
    "    }\n",
    "    init_kwargs.update(kwargs)\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        **init_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module, kwargs: Dict = {}):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        if i == 3:\n",
    "            X = torch.cat([X_full[0], X])\n",
    "            y = torch.cat([torch.zeros(n_samples), y])\n",
    "        acc = evaluate(model, X, y, device=device, batch_size=batch_size, **kwargs)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def run(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    learn_A: bool,\n",
    "    learn_B: bool,\n",
    "    relearn: bool = False,\n",
    "    train_kwargs: Dict = {},\n",
    "    eval_kwargs: Dict = {},\n",
    "):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(\n",
    "        model, learn_A=learn_A, learn_B=learn_B, relearn=relearn, kwargs=train_kwargs\n",
    "    )\n",
    "    evals[end] = global_eval(model, kwargs=eval_kwargs)\n",
    "    print(\n",
    "        f\"{end}, A: {evals[end][0]:.2f}, B: {evals[end][1]:.2f}, Retain: {evals[end][2]:.2f}\"\n",
    "    )\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str, train_kwargs: Dict = {}, eval_kwargs: Dict = {}):\n",
    "    run(\n",
    "        name,\n",
    "        f\"{name}-A\",\n",
    "        learn_A=True,\n",
    "        learn_B=False,\n",
    "        relearn=True,\n",
    "        train_kwargs=train_kwargs,\n",
    "        eval_kwargs=eval_kwargs,\n",
    "    )\n",
    "    run(\n",
    "        name,\n",
    "        f\"{name}-B\",\n",
    "        learn_A=False,\n",
    "        learn_B=True,\n",
    "        relearn=True,\n",
    "        train_kwargs=train_kwargs,\n",
    "        eval_kwargs=eval_kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = [\n",
    "    {\n",
    "        \"name\": name,\n",
    "        \"A\": result[0],\n",
    "        \"B\": result[1],\n",
    "        \"retain\": result[2],\n",
    "    }\n",
    "    for name, result in evals.items()\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_456006/2371333551.py:106: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    }
   ],
   "source": [
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "    include_scatter: bool = True,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    width = 80\n",
    "    x_min, x_max = -width, width\n",
    "    y_min, y_max = -width, width\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    def plot_gaussian_ellipse(gaussian: Gaussian, n_std: float = 2.5, **kwargs):\n",
    "        import numpy as np\n",
    "        from matplotlib.patches import Ellipse\n",
    "\n",
    "        \"\"\"\n",
    "        Add an n‑σ ellipse of a 2‑D Gaussian (mean, cov) to *ax*.\n",
    "        Extra **kwargs are forwarded to matplotlib.patches.Ellipse.\n",
    "        \"\"\"\n",
    "        ax = plt.gca()\n",
    "        mean = gaussian.mu.cpu().numpy()\n",
    "        cov = gaussian.cov.cpu().numpy()\n",
    "        # Eigen‑decomposition of the covariance matrix\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]  # largest first\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "\n",
    "        # Rotation of the ellipse (deg)\n",
    "        theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "\n",
    "        # Full‑width/height of the ellipse (factor 2 because Ellipse wants diameters)\n",
    "        width, height = 2 * n_std * np.sqrt(vals)\n",
    "\n",
    "        ellipse = Ellipse(\n",
    "            xy=mean,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            angle=theta,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            **kwargs,\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "        return ellipse\n",
    "\n",
    "    def scatter(x: torch.Tensor, y: torch.Tensor, **kwargs):\n",
    "        plt.scatter(\n",
    "            x.cpu(),\n",
    "            y.cpu(),\n",
    "            s=1,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if include_scatter:\n",
    "        scatter(X_full[0][:, 0], X_full[0][:, 1], color=\"blue\", label=\"Null\")\n",
    "        scatter(X_full[1][:, 0], X_full[1][:, 1], color=\"orange\", label=\"A\")\n",
    "        scatter(X_full[2][:, 0], X_full[2][:, 1], color=\"yellow\", label=\"B\")\n",
    "        scatter(X_full[3][:, 0], X_full[3][:, 1], color=\"red\", label=\"Retain\")\n",
    "\n",
    "    # plot_gaussian_ellipse(\n",
    "    #     gaussians[0],\n",
    "    #     edgecolor=\"blue\",\n",
    "    #     label=\"Null\",\n",
    "    # )\n",
    "    # plot_gaussian_ellipse(gaussians[1], edgecolor=\"orange\", label=\"A\")\n",
    "    # plot_gaussian_ellipse(gaussians[2], edgecolor=\"yellow\", label=\"B\")\n",
    "    # plot_gaussian_ellipse(gaussians[3], edgecolor=\"red\", label=\"C\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "base_dir = Path(\"./gmm_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(X_full, learn_A=True, learn_B=True, n_samples=n_samples)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "        include_scatter=name == \"init\",\n",
    "    )\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
