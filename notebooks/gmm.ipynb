{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments \n",
    "\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    LogisticModel,\n",
    "    Uniform,\n",
    "    train,\n",
    "    evaluate,\n",
    "    construct_dataset,\n",
    ")\n",
    "import math\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 3\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 10000\n",
    "dim = 2\n",
    "weight_decay = 0.0\n",
    "weight_delta_penalty = 0.0\n",
    "degree = 6\n",
    "eps = 1e-8\n",
    "\n",
    "loss_type = \"cross_entropy\"\n",
    "\n",
    "def cov_gen(rotate: float = 0.0, x_scale: float = 1.0, y_scale: float = 1.0):\n",
    "    rotate = rotate * (torch.pi / 180)\n",
    "    cov = torch.Tensor([\n",
    "        [x_scale, 0],\n",
    "        [0, y_scale],   \n",
    "    ])\n",
    "    rotate = torch.Tensor([\n",
    "        [math.cos(rotate), -math.sin(rotate)],\n",
    "        [math.sin(rotate), math.cos(rotate)],\n",
    "    ])\n",
    "    return rotate @ cov @ rotate.T\n",
    "\n",
    "gaussians = [\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([0.0, 0.0]),\n",
    "        cov=torch.eye(dim) * 500,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([-4.0, 0]),\n",
    "        cov=cov_gen(\n",
    "            rotate=0,\n",
    "            x_scale=6,\n",
    "            y_scale=3,\n",
    "        )\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([4.0, 0]),\n",
    "        cov=cov_gen(\n",
    "            rotate=0,\n",
    "            x_scale=6,\n",
    "            y_scale=3,\n",
    "        )\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([0.0, 0.0]),\n",
    "        cov=cov_gen(\n",
    "            rotate=0,\n",
    "            x_scale=100,\n",
    "            y_scale=100,\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1250/1250 [00:03<00:00, 401.04it/s, loss=3.46]\n",
      "Epoch 2/3: 100%|██████████| 1250/1250 [00:02<00:00, 440.89it/s, loss=4.62]\n",
      "Epoch 3/3: 100%|██████████| 1250/1250 [00:03<00:00, 389.63it/s, loss=7.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init, A: 1.00, B: 1.00, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1250/1250 [00:03<00:00, 411.34it/s, loss=4.65] \n",
      "Epoch 2/3: 100%|██████████| 1250/1250 [00:03<00:00, 369.18it/s, loss=2.94]\n",
      "Epoch 3/3: 100%|██████████| 1250/1250 [00:03<00:00, 407.15it/s, loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base, A: 0.02, B: 0.02, Retain: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1250/1250 [00:03<00:00, 392.51it/s, loss=4.03]\n",
      "Epoch 2/3: 100%|██████████| 1250/1250 [00:02<00:00, 476.99it/s, loss=4.62]\n",
      "Epoch 3/3: 100%|██████████| 1250/1250 [00:02<00:00, 461.98it/s, loss=3.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial, A: 0.12, B: 0.95, Retain: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1250/1250 [00:02<00:00, 458.54it/s, loss=5.76]\n",
      "Epoch 2/3: 100%|██████████| 1250/1250 [00:02<00:00, 463.32it/s, loss=2.32] \n",
      "Epoch 3/3: 100%|██████████| 1250/1250 [00:02<00:00, 470.24it/s, loss=5.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu, A: 0.03, B: 0.01, Retain: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 313/313 [00:00<00:00, 474.86it/s, loss=0.0904] \n",
      "Epoch 2/3: 100%|██████████| 313/313 [00:00<00:00, 463.85it/s, loss=0.0542] \n",
      "Epoch 3/3: 100%|██████████| 313/313 [00:00<00:00, 465.51it/s, loss=0.117]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A, A: 0.99, B: 0.94, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 313/313 [00:00<00:00, 466.50it/s, loss=0.165]  \n",
      "Epoch 2/3: 100%|██████████| 313/313 [00:00<00:00, 477.43it/s, loss=0.0586]  \n",
      "Epoch 3/3: 100%|██████████| 313/313 [00:00<00:00, 479.69it/s, loss=0.0809]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B, A: 1.00, B: 1.00, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 313/313 [00:00<00:00, 482.86it/s, loss=0.265]  \n",
      "Epoch 2/3: 100%|██████████| 313/313 [00:00<00:00, 478.98it/s, loss=0.324]  \n",
      "Epoch 3/3: 100%|██████████| 313/313 [00:00<00:00, 478.15it/s, loss=0.0645] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A, A: 0.97, B: 0.90, Retain: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 313/313 [00:00<00:00, 470.43it/s, loss=0.0519]  \n",
      "Epoch 2/3: 100%|██████████| 313/313 [00:00<00:00, 473.72it/s, loss=0.163]   \n",
      "Epoch 3/3: 100%|██████████| 313/313 [00:00<00:00, 483.43it/s, loss=0.0576] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B, A: 0.31, B: 0.96, Retain: 0.84\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        degree=degree,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(model: nn.Module, learn_A: bool, learn_B: bool, relearn: bool = False, kwargs: Dict = {}):\n",
    "    X, y = construct_dataset(\n",
    "        X_full, learn_A=learn_A, learn_B=learn_B, relearn=relearn, n_samples=n_samples\n",
    "    )\n",
    "    init_kwargs = {\n",
    "        \"eps\": eps,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"device\": device,\n",
    "        \"loss_type\": loss_type,\n",
    "    }\n",
    "    init_kwargs.update(kwargs)\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        **init_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module, kwargs: Dict = {}):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        acc = evaluate(model, X, y, device=device, **kwargs)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False, train_kwargs: Dict = {}, eval_kwargs: Dict = {}):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn, kwargs=train_kwargs)\n",
    "    evals[end] = global_eval(model, kwargs=eval_kwargs)\n",
    "    print(\n",
    "        f\"{end}, A: {evals[end][0]:.2f}, B: {evals[end][1]:.2f}, Retain: {evals[end][2]:.2f}\"\n",
    "    )\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/tcqian/layered-unlearning/venv/lib/python3.12/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    def plot_gaussian_ellipse(gaussian: Gaussian, n_std: float = 2.5, **kwargs):\n",
    "        import numpy as np\n",
    "        from matplotlib.patches import Ellipse\n",
    "\n",
    "        \"\"\"\n",
    "        Add an n‑σ ellipse of a 2‑D Gaussian (mean, cov) to *ax*.\n",
    "        Extra **kwargs are forwarded to matplotlib.patches.Ellipse.\n",
    "        \"\"\"\n",
    "        ax = plt.gca()\n",
    "        mean = gaussian.mu.cpu().numpy()\n",
    "        cov = gaussian.cov.cpu().numpy()\n",
    "        # Eigen‑decomposition of the covariance matrix\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]  # largest first\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "\n",
    "        # Rotation of the ellipse (deg)\n",
    "        theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "\n",
    "        # Full‑width/height of the ellipse (factor 2 because Ellipse wants diameters)\n",
    "        width, height = 2 * n_std * np.sqrt(vals)\n",
    "\n",
    "        ellipse = Ellipse(\n",
    "            xy=mean,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            angle=theta,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            **kwargs,\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "        return ellipse\n",
    "\n",
    "    plot_gaussian_ellipse(\n",
    "        gaussians[0],\n",
    "        edgecolor=\"blue\",\n",
    "        label=\"Null\",\n",
    "    )\n",
    "    plot_gaussian_ellipse(gaussians[1], edgecolor=\"orange\", label=\"A\")\n",
    "    plot_gaussian_ellipse(gaussians[2], edgecolor=\"yellow\", label=\"B\")\n",
    "    plot_gaussian_ellipse(gaussians[3], edgecolor=\"red\", label=\"C\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "base_dir = Path(\"./gmm_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(X_full, learn_A=True, learn_B=True, n_samples=n_samples)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "    )\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
