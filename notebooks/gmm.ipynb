{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments \n",
    "\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    GaussianMixture,\n",
    "    LogisticModel,\n",
    "    Uniform,\n",
    "    train,\n",
    "    evaluate,\n",
    "    construct_dataset,\n",
    ")\n",
    "import math\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 3\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 5000\n",
    "dim = 2\n",
    "weight_decay = 0.0\n",
    "weight_delta_penalty = 0.0\n",
    "\n",
    "rbf = False\n",
    "degree = 2\n",
    "\n",
    "eps = 1e-8\n",
    "n_layers = 0\n",
    "batch_norm = True\n",
    "hidden_dim = 128\n",
    "\n",
    "loss_type = \"cross_entropy\"\n",
    "\n",
    "def ellipse(rotate: float = 0.0, x_scale: float = 1.0, y_scale: float = 1.0):\n",
    "    rotate = rotate * (torch.pi / 180)\n",
    "    cov = torch.Tensor([\n",
    "        [x_scale, 0],\n",
    "        [0, y_scale],   \n",
    "    ])\n",
    "    rotate = torch.Tensor([\n",
    "        [math.cos(rotate), -math.sin(rotate)],\n",
    "        [math.sin(rotate), math.cos(rotate)],\n",
    "    ])\n",
    "    return rotate @ cov @ rotate.T\n",
    "\n",
    "def mu_gen():\n",
    "    width = 50\n",
    "    return torch.rand((dim,)) * 2 * width - width \n",
    "\n",
    "\n",
    "def cov_gen():\n",
    "    base = torch.eye(dim) * 4\n",
    "    U = torch.randn((dim, dim))\n",
    "    perturb = U.T @ U * 0.1\n",
    "    return base + perturb\n",
    "\n",
    "\n",
    "def get_gaussian_mixture(\n",
    "    n_classes: int,\n",
    ") -> GaussianMixture:\n",
    "    classes = []\n",
    "    for i in range(n_classes):\n",
    "        classes.append(\n",
    "            Gaussian(\n",
    "                mu=mu_gen(),\n",
    "                cov=cov_gen(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    mixture = GaussianMixture(\n",
    "        classes=classes,\n",
    "        weights=torch.ones(n_classes) / n_classes,\n",
    "    )\n",
    "    return mixture\n",
    "\n",
    "uniform_half_width = 200\n",
    "task_half_width = 2\n",
    "task_height = 1.5\n",
    "\n",
    "task_rotate = 30\n",
    "task_x_scale = 8\n",
    "task_y_scale = 3\n",
    "\n",
    "null_height = -5\n",
    "null_x_scale = 2\n",
    "null_y_scale = 12\n",
    "\n",
    "\"\"\"\n",
    "Priors\n",
    "- Symmetric about y-axis\n",
    "- Uniform is constant\n",
    "\"\"\"\n",
    "\n",
    "gaussians = [\n",
    "    Uniform(\n",
    "        low=torch.tensor([-1.0, -1.0]) * uniform_half_width,\n",
    "        high=torch.tensor([1.0, 1.0]) * uniform_half_width\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([-task_half_width, task_height]),\n",
    "        cov=ellipse(rotate=-task_rotate, x_scale=task_x_scale, y_scale=task_y_scale)\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([task_half_width, task_height]),\n",
    "        cov=ellipse(rotate=task_rotate, x_scale=task_x_scale, y_scale=task_y_scale)\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([0, null_height]),\n",
    "        cov=ellipse(rotate=0, x_scale=null_x_scale, y_scale=null_y_scale)\n",
    "    ),\n",
    "]\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 438.43it/s, loss=5.23] \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 536.24it/s, loss=5.76]\n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 542.36it/s, loss=3.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init, A: 1.00, B: 1.00, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 585.72it/s, loss=6.01]\n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 586.79it/s, loss=2.08] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 586.04it/s, loss=2.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base, A: 0.14, B: 0.20, Retain: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 582.26it/s, loss=5.14]\n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 579.79it/s, loss=2.21] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 588.82it/s, loss=3.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial, A: 0.34, B: 0.84, Retain: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:01<00:00, 605.21it/s, loss=3.6] \n",
      "Epoch 2/3: 100%|██████████| 625/625 [00:01<00:00, 601.37it/s, loss=3.03] \n",
      "Epoch 3/3: 100%|██████████| 625/625 [00:01<00:00, 605.04it/s, loss=3.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu, A: 0.14, B: 0.27, Retain: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 590.97it/s, loss=0.912]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 574.19it/s, loss=0.343]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 591.57it/s, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A, A: 0.80, B: 0.45, Retain: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 611.85it/s, loss=0.99] \n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 606.48it/s, loss=0.504]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 582.09it/s, loss=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B, A: 0.62, B: 0.78, Retain: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 589.63it/s, loss=1.74]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 587.36it/s, loss=0.772]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 595.05it/s, loss=0.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A, A: 0.65, B: 0.64, Retain: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 157/157 [00:00<00:00, 592.18it/s, loss=0.866]\n",
      "Epoch 2/3: 100%|██████████| 157/157 [00:00<00:00, 592.85it/s, loss=0.313]\n",
      "Epoch 3/3: 100%|██████████| 157/157 [00:00<00:00, 602.86it/s, loss=0.759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B, A: 0.33, B: 0.73, Retain: 0.92\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        degree=degree,\n",
    "        rbf=rbf,\n",
    "        n_layers=n_layers,\n",
    "        batch_norm=batch_norm,\n",
    "        hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(model: nn.Module, learn_A: bool, learn_B: bool, relearn: bool = False, kwargs: Dict = {}):\n",
    "    X, y = construct_dataset(\n",
    "        X_full, learn_A=learn_A, learn_B=learn_B, relearn=relearn, n_samples=n_samples\n",
    "    )\n",
    "    init_kwargs = {\n",
    "        \"eps\": eps,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"device\": device,\n",
    "        \"loss_type\": loss_type,\n",
    "    }\n",
    "    init_kwargs.update(kwargs)\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        **init_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module, kwargs: Dict = {}):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        acc = evaluate(model, X, y, device=device, **kwargs)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False, train_kwargs: Dict = {}, eval_kwargs: Dict = {}):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn, kwargs=train_kwargs)\n",
    "    evals[end] = global_eval(model, kwargs=eval_kwargs)\n",
    "    print(\n",
    "        f\"{end}, A: {evals[end][0]:.2f}, B: {evals[end][1]:.2f}, Retain: {evals[end][2]:.2f}\"\n",
    "    )\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = [\n",
    "    {\n",
    "        \"name\": name,\n",
    "        \"A\": result[0],\n",
    "        \"B\": result[1],\n",
    "        \"retain\": result[2],\n",
    "    } for name, result in evals.items()\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/tcqian/layered-unlearning/venv/lib/python3.12/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    width = 20\n",
    "    x_min, x_max = -width, width\n",
    "    y_min, y_max = -width, width\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    def plot_gaussian_ellipse(gaussian: Gaussian, n_std: float = 2.5, **kwargs):\n",
    "        import numpy as np\n",
    "        from matplotlib.patches import Ellipse\n",
    "\n",
    "        \"\"\"\n",
    "        Add an n‑σ ellipse of a 2‑D Gaussian (mean, cov) to *ax*.\n",
    "        Extra **kwargs are forwarded to matplotlib.patches.Ellipse.\n",
    "        \"\"\"\n",
    "        ax = plt.gca()\n",
    "        mean = gaussian.mu.cpu().numpy()\n",
    "        cov = gaussian.cov.cpu().numpy()\n",
    "        # Eigen‑decomposition of the covariance matrix\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]  # largest first\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "\n",
    "        # Rotation of the ellipse (deg)\n",
    "        theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "\n",
    "        # Full‑width/height of the ellipse (factor 2 because Ellipse wants diameters)\n",
    "        width, height = 2 * n_std * np.sqrt(vals)\n",
    "\n",
    "        ellipse = Ellipse(\n",
    "            xy=mean,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            angle=theta,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            **kwargs,\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "        return ellipse\n",
    "    \n",
    "    def scatter(x: torch.Tensor, y: torch.Tensor, **kwargs):\n",
    "        plt.scatter(\n",
    "            x.cpu(),\n",
    "            y.cpu(),\n",
    "            s=1,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    scatter(X_full[0][:, 0], X_full[0][:, 1], color=\"blue\", label=\"Null\")\n",
    "    # scatter(X_full[1][:, 0], X_full[1][:, 1], color=\"orange\", label=\"A\")\n",
    "    # scatter(X_full[2][:, 0], X_full[2][:, 1], color=\"yellow\", label=\"B\")\n",
    "    # scatter(X_full[3][:, 0], X_full[3][:, 1], color=\"red\", label=\"Retain\")\n",
    "\n",
    "\n",
    "    # plot_gaussian_ellipse(\n",
    "    #     gaussians[0],\n",
    "    #     edgecolor=\"blue\",\n",
    "    #     label=\"Null\",\n",
    "    # )\n",
    "    plot_gaussian_ellipse(gaussians[1], edgecolor=\"orange\", label=\"A\")\n",
    "    plot_gaussian_ellipse(gaussians[2], edgecolor=\"yellow\", label=\"B\")\n",
    "    plot_gaussian_ellipse(gaussians[3], edgecolor=\"red\", label=\"C\")\n",
    "\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "base_dir = Path(\"./gmm_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(X_full, learn_A=True, learn_B=True, n_samples=n_samples)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "    )\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
