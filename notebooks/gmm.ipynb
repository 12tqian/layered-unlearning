{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments \n",
    "\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    LogisticModel,\n",
    "    train,\n",
    "    evaluate,\n",
    "    construct_dataset,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 2\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 10000\n",
    "dim = 2\n",
    "weight_decay = 1e-3\n",
    "degree = 2\n",
    "eps = 1e-8\n",
    "scale = 1.5\n",
    "large_scale = 100\n",
    "\n",
    "loss_type = \"cross_entropy\"\n",
    "\n",
    "# Null, A, B, C\n",
    "gaussians = [\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([0.0, 0.0]),\n",
    "        cov=torch.eye(dim) * scale * large_scale,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([-2.0, 1.5]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([2.0, 1.5]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(mu=torch.tensor([0.0, -3.0]), cov=torch.eye(dim) * scale),\n",
    "]\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 565.29it/s, loss=0.349]\n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 496.03it/s, loss=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init, A: 1.00, B: 1.00, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 553.81it/s, loss=0.265] \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 522.31it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base, A: 0.06, B: 0.06, Retain: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 534.10it/s, loss=0.216]\n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 530.90it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial, A: 0.12, B: 0.95, Retain: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 527.31it/s, loss=0.212] \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 566.03it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu, A: 0.04, B: 0.06, Retain: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 510.46it/s, loss=0.449]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 517.70it/s, loss=0.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A, A: 0.98, B: 0.20, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 519.00it/s, loss=0.541]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 531.05it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B, A: 0.20, B: 0.98, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 524.21it/s, loss=1.16]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 516.36it/s, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A, A: 0.95, B: 0.19, Retain: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 520.73it/s, loss=0.647]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 520.14it/s, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B, A: 0.11, B: 0.94, Retain: 1.00\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        degree=degree,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(model: nn.Module, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    X, y = construct_dataset(\n",
    "        X_full, learn_A=learn_A, learn_B=learn_B, relearn=relearn, n_samples=n_samples\n",
    "    )\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        eps=eps,\n",
    "        n_epochs=n_epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device,\n",
    "        loss_type=loss_type,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        acc = evaluate(model, X, y, device=device)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    evals[end] = global_eval(model)\n",
    "    print(\n",
    "        f\"{end}, A: {evals[end][0]:.2f}, B: {evals[end][1]:.2f}, Retain: {evals[end][2]:.2f}\"\n",
    "    )\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    def plot_gaussian_ellipse(gaussian: Gaussian, n_std: float = 2.5, **kwargs):\n",
    "        import numpy as np\n",
    "        from matplotlib.patches import Ellipse\n",
    "\n",
    "        \"\"\"\n",
    "        Add an n‑σ ellipse of a 2‑D Gaussian (mean, cov) to *ax*.\n",
    "        Extra **kwargs are forwarded to matplotlib.patches.Ellipse.\n",
    "        \"\"\"\n",
    "        ax = plt.gca()\n",
    "        mean = gaussian.mu.cpu().numpy()\n",
    "        cov = gaussian.cov.cpu().numpy()\n",
    "        # Eigen‑decomposition of the covariance matrix\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]  # largest first\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "\n",
    "        # Rotation of the ellipse (deg)\n",
    "        theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "\n",
    "        # Full‑width/height of the ellipse (factor 2 because Ellipse wants diameters)\n",
    "        width, height = 2 * n_std * np.sqrt(vals)\n",
    "\n",
    "        ellipse = Ellipse(\n",
    "            xy=mean,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            angle=theta,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            **kwargs,\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "        return ellipse\n",
    "\n",
    "    plot_gaussian_ellipse(\n",
    "        gaussians[0],\n",
    "        edgecolor=\"blue\",\n",
    "        label=\"Null\",\n",
    "    )\n",
    "    plot_gaussian_ellipse(gaussians[1], edgecolor=\"orange\", label=\"A\")\n",
    "    plot_gaussian_ellipse(gaussians[2], edgecolor=\"yellow\", label=\"B\")\n",
    "    plot_gaussian_ellipse(gaussians[3], edgecolor=\"red\", label=\"C\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "base_dir = Path(\"./gmm_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(X_full, learn_A=True, learn_B=True, n_samples=n_samples)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
