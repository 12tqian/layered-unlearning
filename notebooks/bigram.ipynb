{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Modeling Experiments\n",
    "\n",
    "We experiment with one-layer attention-only transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from layered_unlearning.bigram_modeling import (\n",
    "    Transformer,\n",
    "    get_dataset,\n",
    "    get_transition_matrix,\n",
    ")\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from layered_unlearning.utils import set_seed\n",
    "\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts\n",
    "Below are scripts for training and evaluating our models. Relearning is included in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_list_mask(tensor: torch.Tensor, values_list: List):\n",
    "    \"\"\"\n",
    "    Get a mask for the elements in tensor that are in values_list\n",
    "    \"\"\"\n",
    "    values = torch.tensor(values_list, device=tensor.device, dtype=tensor.dtype)\n",
    "    comparison = tensor.flatten().unsqueeze(1) == values.unsqueeze(0)\n",
    "    mask = comparison.any(dim=1).reshape(tensor.shape)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: Transformer,\n",
    "    learn_A: bool,\n",
    "    learn_B: bool,\n",
    "    lr: float = 1e-3,\n",
    "    batch_size: int = 32,\n",
    "    weight_decay: float = 0.0,\n",
    "    n_epochs: int = 1,\n",
    "    seq_len: int = 24,\n",
    "    length: int = 10000,\n",
    "    device: str = \"cuda\",\n",
    "    relearn: bool = False,\n",
    "    epsilon: float = 0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    learn_A: Whether a -> c\n",
    "    learn_B: Whether b -> c\n",
    "    relearn: Whether to only relearn for learn_A and learn_B\n",
    "    \"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Generate dataset with the correct bigram transition matrix\n",
    "    dataset = get_dataset(\n",
    "        learn_A, learn_B, seq_len=seq_len, length=length, epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch in (pbar := tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)\n",
    "            labels = batch\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            if relearn:\n",
    "                # If we wish to relearn, only compute loss for the relearned tokens\n",
    "                relearn_list = []\n",
    "                if learn_A:\n",
    "                    relearn_list.append(0)\n",
    "                if learn_B:\n",
    "                    relearn_list.append(1)\n",
    "\n",
    "                original_labels = labels[..., :-1].contiguous()\n",
    "\n",
    "                mask = get_in_list_mask(original_labels, relearn_list)\n",
    "                shift_labels = torch.where(mask, shift_labels, -100)\n",
    "\n",
    "                loss = criterion(shift_logits.view(-1, 3), shift_labels.view(-1))\n",
    "            else:\n",
    "                loss = criterion(shift_logits.view(-1, 3), shift_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: Transformer,\n",
    "    seq_len: int = 32,\n",
    "    length: int = 10000,\n",
    "    device: str = \"cuda\",\n",
    "    batch_size: int = 32,\n",
    "    epsilon: float = 0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the margnial transition matrix for model on uniform random data.\n",
    "    \"\"\"\n",
    "    dataset = get_dataset(\n",
    "        learn_A=False, learn_B=False, seq_len=seq_len, length=length, epsilon=epsilon\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize transition matrix and counts\n",
    "    transition_sums = torch.zeros(3, 3, device=device)\n",
    "    token_counts = torch.zeros(3, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Get actual tokens from batch\n",
    "            # Assuming batch contains one-hot encoded tokens, convert to indices\n",
    "            # Get model's predicted probabilities for next tokens\n",
    "            labels = batch\n",
    "            logits = model(batch)\n",
    "            next_token_probs = torch.nn.functional.softmax(\n",
    "                logits, dim=-1\n",
    "            )  # Shape: [batch_size, seq_len, n_vocab]\n",
    "\n",
    "            start_ids = labels[:, :-1].flatten()\n",
    "            next_token_probs = next_token_probs[:, :-1, :].reshape(-1, 3)\n",
    "\n",
    "            transition_sums[start_ids] += next_token_probs\n",
    "            token_counts[start_ids] += 1\n",
    "\n",
    "    # Compute average transition probabilities\n",
    "    # Avoid division by zero for tokens that never appear\n",
    "    token_counts = token_counts.unsqueeze(1)\n",
    "    token_counts[token_counts == 0] = 1\n",
    "\n",
    "    transition_matrix = transition_sums / token_counts\n",
    "\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, there are 2 heads and 1 layer to our transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset hyperparameters\n",
    "seq_len = 8\n",
    "length = 10000\n",
    "epsilon = 0.05\n",
    "\n",
    "# Model hyperparameters\n",
    "n_head = 2\n",
    "n_layers = 1\n",
    "d_model = n_head * 16\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 1e-3\n",
    "n_epochs = 2\n",
    "weight_decay = 0\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. We then perform ablations by subbing parts of the LU version into the base model to see what parts of the model contribute to robustness against adversarial relearning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 176.78it/s, loss=0.529]\n",
      "100%|██████████| 313/313 [00:01<00:00, 204.12it/s, loss=0.576]\n",
      "100%|██████████| 313/313 [00:00<00:00, 314.82it/s, loss=1.05]\n",
      "100%|██████████| 313/313 [00:01<00:00, 188.17it/s, loss=1.02] \n",
      "100%|██████████| 313/313 [00:01<00:00, 179.66it/s, loss=0.761]\n",
      "100%|██████████| 313/313 [00:01<00:00, 178.15it/s, loss=0.831]\n",
      "100%|██████████| 313/313 [00:01<00:00, 176.60it/s, loss=1.01] \n",
      "100%|██████████| 313/313 [00:01<00:00, 302.42it/s, loss=1.06]\n",
      "100%|██████████| 313/313 [00:01<00:00, 193.79it/s, loss=0.513]\n",
      "100%|██████████| 313/313 [00:00<00:00, 348.00it/s, loss=0.375]\n",
      "100%|██████████| 313/313 [00:01<00:00, 193.90it/s, loss=0.223]\n",
      "100%|██████████| 313/313 [00:01<00:00, 197.72it/s, loss=0.363]\n",
      "100%|██████████| 313/313 [00:01<00:00, 235.96it/s, loss=0.273]\n",
      "100%|██████████| 313/313 [00:01<00:00, 249.34it/s, loss=0.386]\n",
      "100%|██████████| 313/313 [00:01<00:00, 174.76it/s, loss=0.491]\n",
      "100%|██████████| 313/313 [00:01<00:00, 181.80it/s, loss=0.581]\n",
      "100%|██████████| 313/313 [00:01<00:00, 195.43it/s, loss=0.531]\n",
      "100%|██████████| 313/313 [00:00<00:00, 358.84it/s, loss=0.22] \n",
      "100%|██████████| 313/313 [00:00<00:00, 344.13it/s, loss=0.366]\n",
      "100%|██████████| 313/313 [00:01<00:00, 194.86it/s, loss=0.419]\n",
      "100%|██████████| 313/313 [00:01<00:00, 194.44it/s, loss=0.178]\n",
      "100%|██████████| 313/313 [00:01<00:00, 192.41it/s, loss=0.376]\n",
      "100%|██████████| 313/313 [00:00<00:00, 349.21it/s, loss=0.392]\n",
      "100%|██████████| 313/313 [00:01<00:00, 195.60it/s, loss=0.316]\n",
      "100%|██████████| 313/313 [00:01<00:00, 221.44it/s, loss=0.28] \n",
      "100%|██████████| 313/313 [00:01<00:00, 192.49it/s, loss=0.564]\n",
      "100%|██████████| 313/313 [00:01<00:00, 195.69it/s, loss=0.462]\n",
      "100%|██████████| 313/313 [00:00<00:00, 349.82it/s, loss=0.453]\n",
      "100%|██████████| 313/313 [00:00<00:00, 338.13it/s, loss=0.459]\n",
      "100%|██████████| 313/313 [00:01<00:00, 190.57it/s, loss=0.206]\n",
      "100%|██████████| 313/313 [00:01<00:00, 192.46it/s, loss=0.31] \n",
      "100%|██████████| 313/313 [00:00<00:00, 340.30it/s, loss=0.221]\n",
      "100%|██████████| 313/313 [00:00<00:00, 345.47it/s, loss=0.419]\n",
      "100%|██████████| 313/313 [00:01<00:00, 196.96it/s, loss=0.298]\n",
      "100%|██████████| 313/313 [00:01<00:00, 190.56it/s, loss=0.206]\n",
      "100%|██████████| 313/313 [00:01<00:00, 212.06it/s, loss=0.319]\n",
      "100%|██████████| 313/313 [00:00<00:00, 347.18it/s, loss=0.598]\n",
      "100%|██████████| 313/313 [00:01<00:00, 192.42it/s, loss=0.393]\n",
      "100%|██████████| 313/313 [00:01<00:00, 191.47it/s, loss=0.41] \n",
      "100%|██████████| 313/313 [00:01<00:00, 203.13it/s, loss=0.17] \n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: Transformer = None):\n",
    "    model = Transformer(\n",
    "        n_vocab=3,\n",
    "        d_model=d_model,\n",
    "        n_layers=n_layers,\n",
    "        n_heads=n_head,\n",
    "        seq_len=seq_len,\n",
    "    ).to(device)\n",
    "\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_train(\n",
    "    model: Transformer, learn_A: bool, learn_B: bool, relearn: bool = False\n",
    "):\n",
    "    model = train(\n",
    "        model,\n",
    "        learn_A=learn_A,\n",
    "        learn_B=learn_B,\n",
    "        n_epochs=n_epochs,\n",
    "        seq_len=seq_len,\n",
    "        length=length,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        epsilon=epsilon,\n",
    "        relearn=relearn,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: Transformer):\n",
    "    return evaluate(\n",
    "        model,\n",
    "        seq_len=seq_len,\n",
    "        length=length,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        epsilon=epsilon,\n",
    "    )\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    evals[end] = global_eval(model)\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def substitute_circuits(\n",
    "    name: str,\n",
    "    base: str,\n",
    "    new: str,\n",
    "    qk_circuit: bool = False,\n",
    "    ov_circuit: bool = False,\n",
    "    ue_circuit: bool = False,\n",
    "):\n",
    "    base_model = deepcopy(model_checkpoints[base])\n",
    "    new_model = deepcopy(model_checkpoints[new])\n",
    "\n",
    "    model = get_model(base_model)\n",
    "    if ue_circuit:\n",
    "        model.embedding.embedding_matrix.weight = (\n",
    "            new_model.embedding.embedding_matrix.weight\n",
    "        )\n",
    "        model.unembedding.weight = new_model.unembedding.weight\n",
    "\n",
    "    if qk_circuit:\n",
    "        for i in range(n_layers):\n",
    "            model.decoder_layers[i].W_k.weight = new_model.decoder_layers[i].W_k.weight\n",
    "            model.decoder_layers[i].W_q.weight = new_model.decoder_layers[i].W_q.weight\n",
    "\n",
    "    if ov_circuit:\n",
    "        for i in range(n_layers):\n",
    "            model.decoder_layers[i].W_o.weight = new_model.decoder_layers[i].W_o.weight\n",
    "            model.decoder_layers[i].W_v.weight = new_model.decoder_layers[i].W_v.weight\n",
    "\n",
    "    model_checkpoints[name] = deepcopy(model)\n",
    "    evals[name] = global_eval(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "\n",
    "for qk_circuit in [0, 1]:\n",
    "    for ov_circuit in [0, 1]:\n",
    "        for ue_circuit in [0, 1]:\n",
    "            name = f\"base-qk-ov-ue-{qk_circuit}{ov_circuit}{ue_circuit}\"\n",
    "            substitute_circuits(\n",
    "                name,\n",
    "                \"base\",\n",
    "                \"base-lu\",\n",
    "                qk_circuit=qk_circuit,\n",
    "                ov_circuit=ov_circuit,\n",
    "                ue_circuit=ue_circuit,\n",
    "            )\n",
    "            run_relearn(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relearned performance\n",
    "We display the performance of relearned models. We want the relearned accuracy on the set not being relearned to be low. \n",
    "\n",
    "Note that models ending with \"000\" are the ones that use all circuits from the base model, and models ending with \"111\" are the ones that use all circuits from the Layered Unlearning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model    A    B Retain\n",
      "5   base-qk-ov-ue-000-A 0.89 0.75   0.04\n",
      "6   base-qk-ov-ue-000-B 0.80 0.89   0.03\n",
      "8   base-qk-ov-ue-001-A 0.89 0.77   0.04\n",
      "9   base-qk-ov-ue-001-B 0.83 0.91   0.04\n",
      "11  base-qk-ov-ue-010-A 0.88 0.64   0.04\n",
      "12  base-qk-ov-ue-010-B 0.68 0.91   0.02\n",
      "14  base-qk-ov-ue-011-A 0.90 0.70   0.04\n",
      "15  base-qk-ov-ue-011-B 0.67 0.89   0.03\n",
      "17  base-qk-ov-ue-100-A 0.89 0.63   0.06\n",
      "18  base-qk-ov-ue-100-B 0.72 0.89   0.05\n",
      "20  base-qk-ov-ue-101-A 0.90 0.65   0.04\n",
      "21  base-qk-ov-ue-101-B 0.67 0.88   0.03\n",
      "23  base-qk-ov-ue-110-A 0.90 0.55   0.03\n",
      "24  base-qk-ov-ue-110-B 0.50 0.90   0.02\n",
      "26  base-qk-ov-ue-111-A 0.90 0.53   0.06\n",
      "27  base-qk-ov-ue-111-B 0.51 0.92   0.05\n"
     ]
    }
   ],
   "source": [
    "def tv_distance(x: torch.Tensor, y: torch.Tensor):\n",
    "    x = x.to(y.device)\n",
    "    return 0.5 * torch.sum(torch.abs(x - y))\n",
    "\n",
    "\n",
    "unlearned_transition_matrix = get_transition_matrix(\n",
    "    learn_A=False, learn_B=False, epsilon=epsilon\n",
    ")\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for key, matrix in evals.items():\n",
    "    task_A_performance = matrix[0, 2].item()\n",
    "    task_B_performance = matrix[1, 2].item()\n",
    "    retain_performance = tv_distance(matrix[2], unlearned_transition_matrix[2]).item()\n",
    "    data.append((key, task_A_performance, task_B_performance, retain_performance))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\"Model\", \"A\", \"B\", \"Retain\"],\n",
    ")\n",
    "\n",
    "relearned_models = [\n",
    "    name for name in df[\"Model\"].unique() if name.endswith(\"-A\") or name.endswith(\"-B\")\n",
    "]\n",
    "filtered_df = df[df[\"Model\"].isin(relearned_models)]\n",
    "\n",
    "filtered_df = filtered_df.sort_values(\n",
    "    \"Model\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    filtered_df.to_string(\n",
    "        formatters={\n",
    "            \"A\": \"{:.2f}\".format,\n",
    "            \"B\": \"{:.2f}\".format,\n",
    "            \"Retain\": \"{:.2f}\".format,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearned performance\n",
    "We display the performance on each task and the retain accuracy for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model    A    B Retain\n",
      "1                base 0.34 0.34   0.03\n",
      "3             base-lu 0.32 0.33   0.01\n",
      "2     base-lu-partial 0.34 0.89   0.02\n",
      "4   base-qk-ov-ue-000 0.34 0.34   0.03\n",
      "7   base-qk-ov-ue-001 0.37 0.38   0.02\n",
      "10  base-qk-ov-ue-010 0.30 0.31   0.02\n",
      "13  base-qk-ov-ue-011 0.31 0.34   0.01\n",
      "16  base-qk-ov-ue-100 0.34 0.33   0.03\n",
      "19  base-qk-ov-ue-101 0.37 0.38   0.02\n",
      "22  base-qk-ov-ue-110 0.30 0.31   0.02\n",
      "25  base-qk-ov-ue-111 0.32 0.33   0.01\n",
      "0                init 0.92 0.92   0.01\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[~df[\"Model\"].isin(relearned_models)]\n",
    "filtered_df = filtered_df.sort_values(\n",
    "    \"Model\",\n",
    ")\n",
    "print(\n",
    "    filtered_df.to_string(\n",
    "        formatters={\n",
    "            \"A\": \"{:.2f}\".format,\n",
    "            \"B\": \"{:.2f}\".format,\n",
    "            \"Retain\": \"{:.2f}\".format,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
