{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments 2\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    LogisticModel,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts\n",
    "Below are scripts for training and evaluating our models. Relearning is included in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X).squeeze()\n",
    "        y_pred = (outputs > 0.5).float()\n",
    "        accuracy = (y_pred == y).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_epochs: int = 1,\n",
    "    lr: float = 0.01,\n",
    "    batch_size: int = 32,\n",
    "    weight_decay: float = 0.01,\n",
    "    device: str = \"cuda\",\n",
    "    eps: float = 1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model using the given data and parameters.\n",
    "    log_1_minus_p: if True, we optimize log(1 - p), otherwise we do gradient ascent.\n",
    "    flip_mask: mask for the data points we want to flip in terms of leanr/unlearn.\n",
    "    mask: mask for the data points we want to use for training, used for relearning.\n",
    "    \"\"\"\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    dataloader = DataLoader(\n",
    "        list(zip(X_train, y_train)),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in (\n",
    "            pbar := tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            loss = -(\n",
    "                batch_y * torch.log(outputs + eps)\n",
    "                + (1 - batch_y) * torch.log(1 - outputs + eps)\n",
    "            )\n",
    "\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 2\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 10000\n",
    "dim = 2\n",
    "weight_decay = 1e-3\n",
    "quadratic_features = True\n",
    "eps = 1e-8\n",
    "scale = 0.15\n",
    "gaussians = [\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([-4.0, 0.0]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([2.0, 1.0]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([2.0, -1.0]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(mu=torch.tensor([4.0, 0.0]), cov=torch.eye(dim) * scale),\n",
    "]\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 480.38it/s, loss=0.00601]\n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 613.97it/s, loss=0.00127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 590.57it/s, loss=0.122] \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 608.67it/s, loss=0.0401] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base [0.0052999998442828655, 0.004999999888241291, 0.9954999685287476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 616.68it/s, loss=0.00982]\n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:01<00:00, 654.38it/s, loss=0.0191] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial [0.02410000003874302, 0.9961999654769897, 0.9942999482154846]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:01<00:00, 631.67it/s, loss=0.0333] \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 608.54it/s, loss=0.0108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu [0.0017999999690800905, 0.001999999862164259, 0.9957000017166138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 617.65it/s, loss=0.0285] \n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 637.45it/s, loss=0.0132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A [0.9960999488830566, 0.5900999903678894, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 648.26it/s, loss=0.00794]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 615.16it/s, loss=0.00363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B [0.5475000143051147, 0.9975999593734741, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 641.25it/s, loss=0.0305]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 643.22it/s, loss=0.0226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A [0.9870999455451965, 0.6082000136375427, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 662.79it/s, loss=0.0282]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 648.30it/s, loss=0.00275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B [0.42149999737739563, 0.9930999875068665, 1.0]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        quadratic_features=quadratic_features,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_dataset(learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    if not relearn:\n",
    "        X.append(X_full[0])\n",
    "        y.append(torch.zeros(n_samples))\n",
    "\n",
    "        X.append(X_full[3])\n",
    "        y.append(torch.ones(n_samples))\n",
    "\n",
    "    if learn_A:\n",
    "        X.append(X_full[1])\n",
    "        y.append(torch.ones(n_samples))\n",
    "    elif not relearn:\n",
    "        X.append(X_full[1])\n",
    "        y.append(torch.zeros(n_samples))\n",
    "\n",
    "    if learn_B:\n",
    "        X.append(X_full[2])\n",
    "        y.append(torch.ones(n_samples))\n",
    "    elif not relearn:\n",
    "        X.append(X_full[2])\n",
    "        y.append(torch.zeros(n_samples))\n",
    "    X = torch.cat(X)\n",
    "    y = torch.cat(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def global_train(model: nn.Module, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    X, y = construct_dataset(learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        eps=eps,\n",
    "        n_epochs=n_epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        acc = evaluate(model, X, y, device=device)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    def scatter(index: int, label: str, color: str):\n",
    "        plt.scatter(\n",
    "            X_full[index][:, 0].cpu(),\n",
    "            X_full[index][:, 1].cpu(),\n",
    "            label=label,\n",
    "            alpha=0.6,\n",
    "            edgecolors=\"k\",\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "    scatter(0, \"Null\", \"blue\")\n",
    "    scatter(1, \"Task A\", \"orange\")\n",
    "    scatter(2, \"Task B\", \"yellow\")\n",
    "    scatter(3, \"Retain\", \"red\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    evals[end] = global_eval(model)\n",
    "    print(end, evals[end])\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/tcqian/layered-unlearning/venv/lib/python3.12/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"./gmm2_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(learn_A=True, learn_B=True)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "    )\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
