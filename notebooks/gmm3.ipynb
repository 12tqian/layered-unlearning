{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Classification Experiments 3\n",
    "\n",
    "\n",
    "We experiment with 2D logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from layered_unlearning.utils import set_seed\n",
    "from layered_unlearning.gmm_classification import (\n",
    "    Gaussian,\n",
    "    LogisticModel,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "seed = set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts\n",
    "Below are scripts for training and evaluating our models. Relearning is included in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X).squeeze()\n",
    "        y_pred = (outputs > 0.5).float()\n",
    "        accuracy = (y_pred == y).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_epochs: int = 1,\n",
    "    lr: float = 0.01,\n",
    "    batch_size: int = 32,\n",
    "    weight_decay: float = 0.01,\n",
    "    device: str = \"cuda\",\n",
    "    eps: float = 1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model using the given data and parameters.\n",
    "    log_1_minus_p: if True, we optimize log(1 - p), otherwise we do gradient ascent.\n",
    "    flip_mask: mask for the data points we want to flip in terms of leanr/unlearn.\n",
    "    mask: mask for the data points we want to use for training, used for relearning.\n",
    "    \"\"\"\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    dataloader = DataLoader(\n",
    "        list(zip(X_train, y_train)),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in (\n",
    "            pbar := tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            loss = -(\n",
    "                batch_y * torch.log(outputs + eps)\n",
    "                + (1 - batch_y) * torch.log(1 - outputs + eps)\n",
    "            )\n",
    "\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Default hyperparameters for our experiments. Of note, we do this in 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_epochs = 2\n",
    "lr = 1e-2\n",
    "batch_size = 32\n",
    "n_classes = 2\n",
    "n_samples = 10000\n",
    "dim = 2\n",
    "weight_decay = 1e-3\n",
    "quadratic_features = True\n",
    "eps = 1e-8\n",
    "scale = 0.15\n",
    "gaussians = [\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([3.0, 0.0]),\n",
    "        cov=torch.eye(dim) * 10,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([2.0, 1.0]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(\n",
    "        mu=torch.tensor([2.0, -1.0]),\n",
    "        cov=torch.eye(dim) * scale,\n",
    "    ),\n",
    "    Gaussian(mu=torch.tensor([4.0, 0.0]), cov=torch.eye(dim) * scale),\n",
    "]\n",
    "\n",
    "# null, task A, task B, retain\n",
    "\n",
    "X_full = [g.sample(n_samples) for g in gaussians]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the initial model, the base unlearned model, and the Layered Unlearning (LU) version of the base unlearned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 568.39it/s, loss=0.0749]\n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 615.81it/s, loss=0.26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init [0.9968999624252319, 0.9960999488830566, 0.9991999864578247]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 554.40it/s, loss=0.21]  \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 584.34it/s, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base [0.023799998685717583, 0.026599999517202377, 0.9402999877929688]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 556.21it/s, loss=0.153] \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 590.66it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-partial [0.04429999738931656, 0.9939999580383301, 0.9803999662399292]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 1250/1250 [00:02<00:00, 541.11it/s, loss=0.194] \n",
      "Epoch 2/2: 100%|██████████| 1250/1250 [00:02<00:00, 558.63it/s, loss=0.229] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu [0.004999999888241291, 0.011299999430775642, 0.9434999823570251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 528.22it/s, loss=0.0155] \n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 595.45it/s, loss=0.00447] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-A [0.9967999458312988, 0.6028000116348267, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 564.99it/s, loss=0.0043] \n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 545.30it/s, loss=0.00293] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-B [0.5651999711990356, 0.9976999759674072, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 535.64it/s, loss=0.0224]\n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 541.09it/s, loss=0.00482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-A [0.9928999543190002, 0.6631999611854553, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 313/313 [00:00<00:00, 556.90it/s, loss=0.018]  \n",
      "Epoch 2/2: 100%|██████████| 313/313 [00:00<00:00, 617.81it/s, loss=0.00591] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base-lu-B [0.446399986743927, 0.9975000023841858, 1.0]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = {}\n",
    "evals = {}\n",
    "\n",
    "\n",
    "def get_model(old_model: nn.Module = None):\n",
    "    model = LogisticModel(\n",
    "        dim=dim,\n",
    "        n_classes=n_classes,\n",
    "        quadratic_features=quadratic_features,\n",
    "    ).to(device)\n",
    "    if old_model is not None:\n",
    "        model.load_state_dict(old_model.state_dict())\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_dataset(learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    if not relearn:\n",
    "        X.append(X_full[0])\n",
    "        y.append(torch.zeros(n_samples))\n",
    "\n",
    "        X.append(X_full[3])\n",
    "        y.append(torch.ones(n_samples))\n",
    "\n",
    "    if learn_A:\n",
    "        X.append(X_full[1])\n",
    "        y.append(torch.ones(n_samples))\n",
    "    elif not relearn:\n",
    "        X.append(X_full[1])\n",
    "        y.append(torch.zeros(n_samples))\n",
    "\n",
    "    if learn_B:\n",
    "        X.append(X_full[2])\n",
    "        y.append(torch.ones(n_samples))\n",
    "    elif not relearn:\n",
    "        X.append(X_full[2])\n",
    "        y.append(torch.zeros(n_samples))\n",
    "    X = torch.cat(X)\n",
    "    y = torch.cat(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def global_train(model: nn.Module, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    X, y = construct_dataset(learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    model = train(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        eps=eps,\n",
    "        n_epochs=n_epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def global_eval(model: nn.Module):\n",
    "    accuracies = []\n",
    "    for i in range(1, 4):\n",
    "        X = X_full[i]\n",
    "        y = torch.ones(n_samples)\n",
    "        acc = evaluate(model, X, y, device=device)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    name: str,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    n_grid: int = 100,\n",
    "    n_samples: int = None,\n",
    "    output_path: Path = None,\n",
    "):\n",
    "    model = model_checkpoints[name]\n",
    "    model.eval()\n",
    "    if n_samples is not None:\n",
    "        if n_samples > X.size(0):\n",
    "            n_samples = X.size(0)\n",
    "        inds = torch.randperm(X.size(0))[:n_samples]\n",
    "        X = X[inds]\n",
    "        y = y[inds]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, n_grid),\n",
    "        torch.linspace(y_min, y_max, n_grid),\n",
    "    )\n",
    "    grid = torch.stack([xx.ravel(), yy.ravel()], dim=1).to(device)\n",
    "    with torch.no_grad():\n",
    "        grid_out = model(grid).squeeze().cpu()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(\n",
    "        xx.cpu(),\n",
    "        yy.cpu(),\n",
    "        grid_out.reshape(xx.shape),\n",
    "        levels=[0, 0.5, 1],\n",
    "        alpha=0.2,\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "\n",
    "    def scatter(index: int, label: str, color: str):\n",
    "        plt.scatter(\n",
    "            X_full[index][:, 0].cpu(),\n",
    "            X_full[index][:, 1].cpu(),\n",
    "            label=label,\n",
    "            alpha=0.6,\n",
    "            edgecolors=\"k\",\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "    scatter(0, \"Null\", \"blue\")\n",
    "    scatter(1, \"Task A\", \"orange\")\n",
    "    scatter(2, \"Task B\", \"yellow\")\n",
    "    scatter(3, \"Retain\", \"red\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def run(start: str, end: str, learn_A: bool, learn_B: bool, relearn: bool = False):\n",
    "    assert start is None or start in model_checkpoints\n",
    "    model = get_model(model_checkpoints.get(start))\n",
    "    model = global_train(model, learn_A=learn_A, learn_B=learn_B, relearn=relearn)\n",
    "    evals[end] = global_eval(model)\n",
    "    print(end, evals[end])\n",
    "    model_checkpoints[end] = deepcopy(model)\n",
    "\n",
    "\n",
    "def run_relearn(name: str):\n",
    "    run(name, f\"{name}-A\", learn_A=True, learn_B=False, relearn=True)\n",
    "    run(name, f\"{name}-B\", learn_A=False, learn_B=True, relearn=True)\n",
    "\n",
    "\n",
    "run(None, \"init\", learn_A=True, learn_B=True)\n",
    "run(\"init\", \"base\", learn_A=False, learn_B=False)\n",
    "run(\"init\", \"base-lu-partial\", learn_A=False, learn_B=True)\n",
    "run(\"base-lu-partial\", \"base-lu\", learn_A=False, learn_B=False)\n",
    "run_relearn(\"base\")\n",
    "run_relearn(\"base-lu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We visualize decision boundaries learned and the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/tcqian/layered-unlearning/venv/lib/python3.12/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"./gmm3_figures\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "X, y = construct_dataset(learn_A=True, learn_B=True)\n",
    "for name in model_checkpoints:\n",
    "    visualize(\n",
    "        name,\n",
    "        X,\n",
    "        y,\n",
    "        n_grid=100,\n",
    "        n_samples=5000,\n",
    "        output_path=base_dir / f\"{name}.png\",\n",
    "    )\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
